# Academic Assistant ğŸ¤–

An AI-powered academic assistant with document management and conversational capabilities using Ollama, MinIO, and Streamlit.

## Features âœ¨

- **Document Storage**: Secure PDF storage with MinIO object storage
- **AI Conversations**: LLM-powered Q&A with context-aware responses
- **Vector Search**: Semantic document retrieval using Ollama embeddings
- **Event-Driven Updates**: Automatic index updates via MinIO webhooks
- **Multi-Model Support**: Flexible integration with various LLM providers

## Prerequisites ğŸ“‹

- Python 3.9+
- [Ollama](https://ollama.ai/) running locally
- MinIO server (included in repository)

## Installation ğŸ› ï¸

```bash
git clone https://github.com/feevemouad/Academic-Assistant.git
cd Academic-Assistant
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

## Configuration âš™ï¸

1. **MinIO Setup**:
   - Create bucket `documents` in MinIO console (http://localhost:9001)
   - Configure event notifications:
     - ARN: `http://localhost:8000/webhook`
     - Events: `Put` and `Delete`

2. **Ollama Setup**:
   ```bash
   ollama pull nomic-embed-text
   ollama pull llama3.1
   ```

3. **Environment Setup**:
   Update `config.yml` if needed (defaults work for local setup):
   ```yaml
   minio:
     endpoint: 'localhost:9000'
     access_key: 'minioadmin'
     secret_key: 'minioadmin'
   ```

## Usage ğŸš€

1. **Start MinIO Server**:
   ```bash
   ./minio.exe server data --console-address ':9001'
   ```

2. **Run Backend API**:
   ```bash
   uvicorn main:app --reload --host 0.0.0.0 --port 8000 --log-level info
   ```

3. **Launch Frontend**:
   ```bash
   cd frontend
   streamlit run main.py --server.headless true
   ```

Access the interface at `http://localhost:8501`

## Project Structure ğŸ“

```
Academic-Assistant/
â”œâ”€â”€ data/               # MinIO storage directory
â”œâ”€â”€ frontend/           # Streamlit UI
â”‚   â”œâ”€â”€ Utils/          # UI components
â”‚   â””â”€â”€ .streamlit/     # Streamlit config
â”œâ”€â”€ src/                # Core logic
â”‚   â”œâ”€â”€ models/         # Data models
â”‚   â”œâ”€â”€ vector_store.py # Document processing
â”‚   â””â”€â”€ chat_model.py   # Conversation logic
â”œâ”€â”€ config.yml          # Configuration
â”œâ”€â”€ requirements.txt    # Dependencies
â””â”€â”€ README.md
```

## API Endpoints ğŸŒ

- `POST /chat` - Process user queries
- `POST /webhook` - Handle MinIO events (auto-configures vector store)

## Troubleshooting ğŸ”§

**Common Issues:**
- Port conflicts: Ensure ports 8000 (API), 8501 (UI), 9000/9001 (MinIO) are free
- MinIO errors: Delete `data/.minio.sys` and restart MinIO
- Model issues: Verify Ollama models are downloaded (`ollama list`)

**Log Locations:**
- API Server: Console output
- MinIO: Console output
- Streamlit: Console output

## Technologies Used ğŸ› ï¸

- **Ollama** - Local LLM & embeddings
- **MinIO** - Object storage
- **LangChain** - LLM orchestration
- **FAISS** - Vector search
- **Streamlit** - Web interface
- **FastAPI** - REST backend

---

**Note:** For production use:
- Secure MinIO with proper credentials
- Enable HTTPS
- Use persistent storage for MinIO data
- Consider adding authentication layers
